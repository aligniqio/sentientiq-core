jobs/enrich_social.py


#!/usr/bin/env python3
import os, json, hashlib, datetime as dt
import pandas as pd
import pyarrow as pa, pyarrow.parquet as pq
import s3fs

S3_RAW = "s3://sentientiq-raw-data/social"
S3_PROC = "s3://sentientiq-processed-data/social"

def sha1(s: str) -> str:
    return hashlib.sha1(s.encode("utf-8", "ignore")).hexdigest()

def detect_lang(text: str) -> str:
    # TODO: plug cld3/fasttext; fallback:
    return "en"

def bot_score_stub(row) -> float:
    # TODO: replace with trained classifier
    score = 0.1
    if row.get("engagement_count", 0) == 0 and len(row.get("text","")) < 20:
        score = 0.6
    return min(1.0, max(0.0, score))

EMOTIONS = ["anticipation","joy","trust","surprise","anger","fear","sadness","disgust"]

def emotion_scores_stub(text: str):
    # TODO: replace with model inference
    base = 0.05
    scores = {e: base for e in EMOTIONS}
    if "launch" in text.lower(): scores["anticipation"] += 0.3
    if "love" in text.lower(): scores["joy"] += 0.3
    if "hate" in text.lower(): scores["anger"] += 0.3
    primary = max(scores, key=scores.get)
    return primary, scores

def brand_map(text: str) -> str:
    # TODO: lexicon/NLP model; stub
    if "sentientiq" in text.lower(): return "SentientIQ"
    return "Unknown"

def simhash_stub(text: str) -> str:
    return sha1("fh-" + text[:64])

def process_batch(df: pd.DataFrame, platform: str, ingest_date: str):
    df = df.copy()
    df["platform"] = platform
    df["ingested_at"] = pd.Timestamp.utcnow()
    df["lang"] = df["text"].apply(detect_lang)
    df["brand"] = df["text"].apply(brand_map)
    df["topic"] = df["text"].apply(lambda t: [])
    df["engagement_count"] = df.get("engagement_count", pd.Series([0]*len(df)))
    df["dedupe_key"] = df.apply(lambda r: sha1((r.get("author_id","") or "") + ":" + (r.get("text","") or "")), axis=1)
    df["fingerprint"] = df["text"].apply(simhash_stub)

    em = df["text"].apply(emotion_scores_stub)
    df["emotion_primary"] = em.apply(lambda t: t[0])
    df["emotion_scores"]  = em.apply(lambda t: t[1])

    df["bot_score"] = df.apply(bot_score_stub, axis=1)
    df["authenticity"] = 1.0 - df["bot_score"]
    # dedupe within day (you can expand to a 30-day rolling via Athena join)
    df["is_duplicate"] = df["dedupe_key"].duplicated(keep="first")

    # partitions
    df["platform_part"] = platform
    df["brand_part"] = df["brand"]
    df["ingest_date"] = ingest_date

    return df

def save_parquet(df: pd.DataFrame, s3_path: str):
    fs = s3fs.S3FileSystem()
    table = pa.Table.from_pandas(df, preserve_index=False)
    with fs.open(s3_path, 'wb') as f:
        pq.write_table(table, f, compression="snappy")

def run(platform: str, ingest_date: str):
    fs = s3fs.S3FileSystem()
    raw_prefix = f"{S3_RAW}/{platform}/ingest_date={ingest_date}/"
    files = fs.glob(raw_prefix + "*.json")
    if not files:
        print("No raw files.")
        return
    rows = []
    for path in files:
        with fs.open(path) as f:
            for line in f:
                rows.append(json.loads(line))
    df = pd.DataFrame(rows)
    # normalize expected columns
    for c in ["post_id","author_id","author_handle","created_at","text","source_url"]:
        if c not in df.columns: df[c] = None
    df["created_at"] = pd.to_datetime(df["created_at"], errors="coerce", utc=True)

    dfp = process_batch(df, platform, ingest_date)
    out = f"{S3_PROC}/platform={platform}/brand={ingest_date}/ingest_date={ingest_date}/part-000.parquet"
    # (brand partition is overwritten by df["brand_part"] values; using brand in dir name is optional)
    out = f"{S3_PROC}/platform={platform}/ingest_date={ingest_date}/part-000.parquet"
    save_parquet(dfp, out)
    print("Wrote", out)

if __name__ == "__main__":
    platform = os.environ.get("PLATFORM","linkedin")
    ingest_date = os.environ.get("INGEST_DATE", dt.date.today().isoformat())
    run(platform, ingest_date)
